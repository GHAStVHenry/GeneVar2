data = list(nP = max(data.change[complete.cases(data.change),]$id),
N_obs = sum(1*complete.cases(data.change)),
N_mis = length(complete.cases(data.change))-
sum(1*complete.cases(data.change)),
nW = ncol(W),
y_obs = data.change[complete.cases(data.change),]$change,
W = W,
S = data.change[complete.cases(data.change),]$id,
index_obs = which(complete.cases(data.change)==TRUE),
index_mis = which(complete.cases(data.change)==FALSE),
N=40),
control = list(max_treedepth = 20),
seed = 1,
cores = 3,
chains = 1,
warmup = 1000,
iter = 10000)
# extract the estimates of the higher-level effects, intercept and effect
pX <- as.data.frame(as.array(post)[,,grep("^coef_means", dimnames(post)$parameters, perl=T)])
# give the effects sensible names
names(pX) <- c("intercept", "rateOfChange")
# hpdi function
HPDIFunct <- function (vector, HPDI = 0.95) {
sortVec <- sort(vector)
ninetyFiveVec <- ceiling(HPDI*length(sortVec))
fiveVec <- length(sortVec) - length(ninetyFiveVec)
diffVec <- sapply(1:fiveVec, function (i) sortVec[i + ninetyFiveVec] - sortVec[i])
minVal <- sortVec[which.min(diffVec)]
maxVal <- sortVec[which.min(diffVec) + ninetyFiveVec]
median <- median(sortVec)
return(list(est = round(c(minVal, median, maxVal),2)))
}
# lower 95% HPDIx, median, and upper 95% HPDI
summaryDF <- as.data.frame(t(unlist(HPDIFunct(pX$intercept))))
summaryDF[2,] <- t(unlist(HPDIFunct(pX$rateOfChange)))
row.names(summaryDF) <- c("Intercept", "period")
names(summaryDF) <- c("lower", "median", "upper")
summaryDF
#### missing model
missing.model <- function(){
data <- read.csv("~/Dropbox/Projects/bayes code/data.csv")
data.change <- data %>%
transmute(id=id,
period.1 = day_0-day_15+rnorm(10,0,0.5),
period.2 = day_15-day_30+rnorm(10,0,0.5),
period.3 = day_30-day_45+rnorm(10,0,0.5),
period.4 = day_45-day_60+rnorm(10,0,0.5))%>%
pivot_longer(!id, names_to = "period", values_to = "change")
data.change
normal_model <- "
data {
int<lower = 0> nP;           // number of participants
int<lower = 0> N_obs;// the number of observations
int<lower = 0> N_mis;// the number of missing observations
int<lower = 0> nW;           // the number of within-subject predictors
vector[N_obs] y_obs;     // vector of observed amount of change
matrix[N_obs, nW] W; // matrix of within-subjects predictors
int S[N_obs];        // integer value of subject associated with each y outcome
int<lower = 1, upper = N_obs + N_mis> index_obs[N_obs];
int<lower = 1, upper = N_obs + N_mis> index_mis[N_mis];
int N;
}
transformed data {
vector[N_obs] y_scaled;
y_scaled = (y_obs - mean(y_obs)) / sd(y_obs);  // standardising via z-transform
}
parameters {
vector[nW] scaled_coef_means;         //scaled coefficients for group means
vector<lower=0>[nW] scaled_coef_sds;
corr_matrix[nW] corr_mat;             // correlation among coefficients
matrix[nP,nW] scaled_subj_coefs;      // scaled subject coefficients
real<lower=0> noise;                  // noise parameter
vector[N_mis] y_mis;     // vector of missing amount of change
}
transformed parameters {
vector[N] y;
y[index_obs] = y_obs;
y[index_mis] = y_mis;
}
model {
// weakly informed priors
scaled_coef_means ~ normal(0,1) ;   // flat gaussian prior on mean of subj coefs
scaled_coef_sds ~ weibull(2,1) ;    // flat prior on sd of subj coefs
corr_mat ~ lkj_corr(1) ;            // flat prior on correlations
noise ~ weibull(2,1) ;              //prior on measuement noise
// likelihood for lower-tier in the hierarchy.
// estimates a matrix of coefficients with as many rows as participants and with as many within-subject coefficients as there are subjects in the model (intercept and effect) for each
for(p in 1:nP){
scaled_subj_coefs[p,] ~ multi_normal(
scaled_coef_means, quad_form_diag(corr_mat, scaled_coef_sds)
) ;
}
// now for the higher tier in the hierarchy
for (i in 1:nP) {
y[i] ~ normal(
sum(scaled_subj_coefs[S[i]].*W[i,]),
noise
);
}
}
generated quantities{
//declare variables
vector[nW] coef_means;   // transform mean coefficients back to original y scale
vector[nW] coef_sds;     // transform sds back to original y scale
// rescale everything
coef_means = scaled_coef_means*sd(y_obs);        // for mean coefs multiply by sd
coef_means[1] = coef_means[1] + mean(y_obs);     // for intercept also add mean
coef_sds = scaled_coef_sds*sd(y_obs);            // for group sds multiply by sd
}
"
# matrix of within-subject predictors
W <- model.matrix(change ~ period, data.change[complete.cases(data.change),])
post <- stan(model_code = normal_model,
data = list(nP = max(data.change[complete.cases(data.change),]$id),
N_obs = sum(1*complete.cases(data.change)),
N_mis = length(complete.cases(data.change))-
sum(1*complete.cases(data.change)),
nW = ncol(W),
y_obs = data.change[complete.cases(data.change),]$change,
W = W,
S = data.change[complete.cases(data.change),]$id,
index_obs = which(complete.cases(data.change)==TRUE),
index_mis = which(complete.cases(data.change)==FALSE),
N=40),
control = list(max_treedepth = 20),
seed = 1,
cores = 3,
chains = 1,
warmup = 1000,
iter = 10000)
# extract the estimates of the higher-level effects, intercept and effect
pX <- as.data.frame(as.array(post)[,,grep("^coef_means", dimnames(post)$parameters, perl=T)])
# give the effects sensible names
names(pX) <- c("intercept", "rateOfChange")
# hpdi function
HPDIFunct <- function (vector, HPDI = 0.95) {
sortVec <- sort(vector)
ninetyFiveVec <- ceiling(HPDI*length(sortVec))
fiveVec <- length(sortVec) - length(ninetyFiveVec)
diffVec <- sapply(1:fiveVec, function (i) sortVec[i + ninetyFiveVec] - sortVec[i])
minVal <- sortVec[which.min(diffVec)]
maxVal <- sortVec[which.min(diffVec) + ninetyFiveVec]
median <- median(sortVec)
return(list(est = round(c(minVal, median, maxVal),2)))
}
# lower 95% HPDIx, median, and upper 95% HPDI
summaryDF <- as.data.frame(t(unlist(HPDIFunct(pX$intercept))))
summaryDF[2,] <- t(unlist(HPDIFunct(pX$rateOfChange)))
row.names(summaryDF) <- c("Intercept", "period")
names(summaryDF) <- c("lower", "median", "upper")
summaryDF
}
data <- read.csv("~/Dropbox/Projects/bayes code/data.csv")
data.change <- data %>%
transmute(id=id,
period.1 = day_0-day_15+rnorm(10,0,0.5),
period.2 = day_15-day_30+rnorm(10,0,0.5),
period.3 = day_30-day_45+rnorm(10,0,0.5),
period.4 = day_45-day_60+rnorm(10,0,0.5))%>%
pivot_longer(!id, names_to = "period", values_to = "change")
data.change
normal_model <- "
data {
int<lower = 0> nP;           // number of participants
int<lower = 0> N_obs;// the number of observations
int<lower = 0> N_mis;// the number of missing observations
int<lower = 0> nW;           // the number of within-subject predictors
vector[N_obs] y_obs;     // vector of observed amount of change
matrix[N_obs, nW] W; // matrix of within-subjects predictors
int S[N_obs];        // integer value of subject associated with each y outcome
int<lower = 1, upper = N_obs + N_mis> index_obs[N_obs];
int<lower = 1, upper = N_obs + N_mis> index_mis[N_mis];
int N;
}
transformed data {
vector[N_obs] y_scaled;
y_scaled = (y_obs - mean(y_obs)) / sd(y_obs);  // standardising via z-transform
}
parameters {
vector[nW] scaled_coef_means;         //scaled coefficients for group means
vector<lower=0>[nW] scaled_coef_sds;
corr_matrix[nW] corr_mat;             // correlation among coefficients
matrix[nP,nW] scaled_subj_coefs;      // scaled subject coefficients
real<lower=0> noise;                  // noise parameter
vector[N_mis] y_mis;     // vector of missing amount of change
}
transformed parameters {
vector[N] y;
y[index_obs] = y_obs;
y[index_mis] = y_mis;
}
model {
// weakly informed priors
scaled_coef_means ~ normal(0,1) ;   // flat gaussian prior on mean of subj coefs
scaled_coef_sds ~ weibull(2,1) ;    // flat prior on sd of subj coefs
corr_mat ~ lkj_corr(1) ;            // flat prior on correlations
noise ~ weibull(2,1) ;              //prior on measuement noise
// likelihood for lower-tier in the hierarchy.
// estimates a matrix of coefficients with as many rows as participants and with as many within-subject coefficients as there are subjects in the model (intercept and effect) for each
for(p in 1:nP){
scaled_subj_coefs[p,] ~ multi_normal(
scaled_coef_means, quad_form_diag(corr_mat, scaled_coef_sds)
) ;
}
// now for the higher tier in the hierarchy
for (i in 1:nP) {
y[i] ~ normal(
sum(scaled_subj_coefs[S[i]].*W[i,]),
noise
);
}
}
generated quantities{
//declare variables
vector[nW] coef_means;   // transform mean coefficients back to original y scale
vector[nW] coef_sds;     // transform sds back to original y scale
// rescale everything
coef_means = scaled_coef_means*sd(y_obs);        // for mean coefs multiply by sd
coef_means[1] = coef_means[1] + mean(y_obs);     // for intercept also add mean
coef_sds = scaled_coef_sds*sd(y_obs);            // for group sds multiply by sd
}
"
# matrix of within-subject predictors
W <- model.matrix(change ~ period, data.change[complete.cases(data.change),])
post <- stan(model_code = normal_model,
data = list(nP = max(data.change[complete.cases(data.change),]$id),
N_obs = sum(1*complete.cases(data.change)),
N_mis = length(complete.cases(data.change))-
sum(1*complete.cases(data.change)),
nW = ncol(W),
y_obs = data.change[complete.cases(data.change),]$change,
W = W,
S = data.change[complete.cases(data.change),]$id,
index_obs = which(complete.cases(data.change)==TRUE),
index_mis = which(complete.cases(data.change)==FALSE),
N=40),
control = list(max_treedepth = 20),
seed = 1,
cores = 3,
chains = 1,
warmup = 1000,
iter = 10000)
# extract the estimates of the higher-level effects, intercept and effect
pX <- as.data.frame(as.array(post)[,,grep("^coef_means", dimnames(post)$parameters, perl=T)])
# give the effects sensible names
names(pX) <- c("intercept", "rateOfChange")
# hpdi function
HPDIFunct <- function (vector, HPDI = 0.95) {
sortVec <- sort(vector)
ninetyFiveVec <- ceiling(HPDI*length(sortVec))
fiveVec <- length(sortVec) - length(ninetyFiveVec)
diffVec <- sapply(1:fiveVec, function (i) sortVec[i + ninetyFiveVec] - sortVec[i])
minVal <- sortVec[which.min(diffVec)]
maxVal <- sortVec[which.min(diffVec) + ninetyFiveVec]
median <- median(sortVec)
return(list(est = round(c(minVal, median, maxVal),2)))
}
# lower 95% HPDIx, median, and upper 95% HPDI
summaryDF <- as.data.frame(t(unlist(HPDIFunct(pX$intercept))))
summaryDF[2,] <- t(unlist(HPDIFunct(pX$rateOfChange)))
row.names(summaryDF) <- c("Intercept", "period")
names(summaryDF) <- c("lower", "median", "upper")
summaryDF
}
data <- read.csv("~/Dropbox/Projects/bayes code/data.csv")
data.change <- data %>%
transmute(id=id,
period.1 = day_0-day_15+rnorm(10,0,0.5),
period.2 = day_15-day_30+rnorm(10,0,0.5),
period.3 = day_30-day_45+rnorm(10,0,0.5),
period.4 = day_45-day_60+rnorm(10,0,0.5))%>%
pivot_longer(!id, names_to = "period", values_to = "change")
data.change
normal_model <- "
data {
int<lower = 0> nP;           // number of participants
int<lower = 0> N_obs;// the number of observations
int<lower = 0> N_mis;// the number of missing observations
int<lower = 0> nW;           // the number of within-subject predictors
vector[N_obs] y_obs;     // vector of observed amount of change
matrix[N_obs, nW] W; // matrix of within-subjects predictors
int S[N_obs];        // integer value of subject associated with each y outcome
int<lower = 1, upper = N_obs + N_mis> index_obs[N_obs];
int<lower = 1, upper = N_obs + N_mis> index_mis[N_mis];
int N;
}
transformed data {
vector[N_obs] y_scaled;
y_scaled = (y_obs - mean(y_obs)) / sd(y_obs);  // standardising via z-transform
}
parameters {
vector[nW] scaled_coef_means;         //scaled coefficients for group means
vector<lower=0>[nW] scaled_coef_sds;
corr_matrix[nW] corr_mat;             // correlation among coefficients
matrix[nP,nW] scaled_subj_coefs;      // scaled subject coefficients
real<lower=0> noise;                  // noise parameter
vector[N_mis] y_mis;     // vector of missing amount of change
}
transformed parameters {
vector[N] y;
y[index_obs] = y_obs;
y[index_mis] = y_mis;
}
model {
// weakly informed priors
scaled_coef_means ~ normal(0,1) ;   // flat gaussian prior on mean of subj coefs
scaled_coef_sds ~ weibull(2,1) ;    // flat prior on sd of subj coefs
corr_mat ~ lkj_corr(1) ;            // flat prior on correlations
noise ~ weibull(2,1) ;              //prior on measuement noise
// likelihood for lower-tier in the hierarchy.
// estimates a matrix of coefficients with as many rows as participants and with as many within-subject coefficients as there are subjects in the model (intercept and effect) for each
for(p in 1:nP){
scaled_subj_coefs[p,] ~ multi_normal(
scaled_coef_means, quad_form_diag(corr_mat, scaled_coef_sds)
) ;
}
// now for the higher tier in the hierarchy
for (i in 1:nP) {
y[i] ~ normal(
sum(scaled_subj_coefs[S[i]].*W[i,]),
noise
);
}
}
generated quantities{
//declare variables
vector[nW] coef_means;   // transform mean coefficients back to original y scale
vector[nW] coef_sds;     // transform sds back to original y scale
// rescale everything
coef_means = scaled_coef_means*sd(y_obs);        // for mean coefs multiply by sd
coef_means[1] = coef_means[1] + mean(y_obs);     // for intercept also add mean
coef_sds = scaled_coef_sds*sd(y_obs);            // for group sds multiply by sd
}
"
# matrix of within-subject predictors
W <- model.matrix(change ~ period, data.change[complete.cases(data.change),])
post <- stan(model_code = normal_model,
data = list(nP = max(data.change[complete.cases(data.change),]$id),
N_obs = sum(1*complete.cases(data.change)),
N_mis = length(complete.cases(data.change))-
sum(1*complete.cases(data.change)),
nW = ncol(W),
y_obs = data.change[complete.cases(data.change),]$change,
W = W,
S = data.change[complete.cases(data.change),]$id,
index_obs = which(complete.cases(data.change)==TRUE),
index_mis = which(complete.cases(data.change)==FALSE),
N=40),
control = list(max_treedepth = 20),
seed = 1,
cores = 3,
chains = 1,
warmup = 1000,
iter = 10000)
# extract the estimates of the higher-level effects, intercept and effect
pX <- as.data.frame(as.array(post)[,,grep("^coef_means", dimnames(post)$parameters, perl=T)])
# give the effects sensible names
names(pX) <- c("intercept", "rateOfChange")
# hpdi function
HPDIFunct <- function (vector, HPDI = 0.95) {
sortVec <- sort(vector)
ninetyFiveVec <- ceiling(HPDI*length(sortVec))
fiveVec <- length(sortVec) - length(ninetyFiveVec)
diffVec <- sapply(1:fiveVec, function (i) sortVec[i + ninetyFiveVec] - sortVec[i])
minVal <- sortVec[which.min(diffVec)]
maxVal <- sortVec[which.min(diffVec) + ninetyFiveVec]
median <- median(sortVec)
return(list(est = round(c(minVal, median, maxVal),2)))
}
# lower 95% HPDIx, median, and upper 95% HPDI
summaryDF <- as.data.frame(t(unlist(HPDIFunct(pX$intercept))))
summaryDF[2,] <- t(unlist(HPDIFunct(pX$rateOfChange)))
row.names(summaryDF) <- c("Intercept", "period")
names(summaryDF) <- c("lower", "median", "upper")
summaryDF
install.packages(c("car", "corrplot", "lmtest", "spatialreg", "spdep", "spgwr", "stargazer", "tidycensus"))
y
#### Library ####
library(tidyverse)
library(plotly)
library(corrplot)
library(car)
library(spdep)
library(nlme)
library(tidycensus)
library(viridis)
library(spatialreg)
library(readxl)
library(spgwr)
FB_combined_comments <- read.csv("~/Downloads/FB_combined_comments.csv", comment.char="#")
fbdata<-FB_combined_comments[1:500,]
View(fbdata)
View(FB_combined_comments)
fbdata$comments_full
setwd("~/Documents/GitHub/GeneVar2/genevarapp")
library(shiny)
library(sveval)
library(VariantAnnotation)
## --------------------------------------
#### launch app locally
runApp()
png.names <- substr(list.files(pattern = "png$", full.names=TRUE),3,100)
png.names <- png.names[which(png.names!="output.png")]
rl = lapply(png.names, png::readPNG)
gl = lapply(rl, grid::rasterGrob)
gl[[1]]$width=100
gl[[1]]$height=100
gridExtra::grid.arrange(grobs=gl,ncol=1)
gl[[1]]
library(shiny)
library(sveval)
library(VariantAnnotation)
## --------------------------------------
#### launch app locally
runApp()
bar_plot<-hist(rnorm(100))
saveRDS(bar_plot, "plot_1.rds")
plot_1 <- readRDS("~/Documents/GitHub/GeneVar2/genevarapp/plot_1.rds")
bar_plot<-hist(rnorm(100))
saveRDS(bar_plot, "plot_2.rds")
rds.names <- substr(list.files(pattern = "rds$", full.names=TRUE),3,100)
rl = lapply(rds.names, readRDS)
ggarrange(rl)
ggarrange(plotlist=rl)
grid.arrange(rl)
lapply(rds.names, ggplotGrob)
ggplotGrob(rl[[1]])
rl[[1]]
plot_1
p1 <- ggplot(mtcars) + geom_point(aes(mpg, disp))
p2 <- ggplot(mtcars) + geom_boxplot(aes(gear, disp, group = gear))
ggplotGrob(p1)
g1<-ggplotGrob(p1)
g2<-ggplotGrob(p2)
plot_grid(p1,p2)
install.packages("cowplot")
library(cowplot)
plot_grid(p1,p2)
plot_grid(p1,p2,plot_1)
plot_grid(rl)
library(shiny)
library(sveval)
library(VariantAnnotation)
## --------------------------------------
#### launch app locally
runApp()
library(shiny)
library(sveval)
library(VariantAnnotation)
## --------------------------------------
#### launch app locally
runApp()
library(shiny)
library(sveval)
library(VariantAnnotation)
## --------------------------------------
#### launch app locally
runApp()
library(shiny)
library(sveval)
library(VariantAnnotation)
## --------------------------------------
#### launch app locally
runApp()
#### launch app on UCSC server
#runApp(port=3457, host='0.0.0.0')
library(shiny)
library(sveval)
library(VariantAnnotation)
## --------------------------------------
#### launch app locally
runApp()
#### launch app on UCSC server
#runApp(port=3457, host='0.0.0.0')
library(shiny)
library(sveval)
library(VariantAnnotation)
## --------------------------------------
#### launch app locally
runApp()
#### launch app on UCSC server
#runApp(port=3457, host='0.0.0.0')
## --------------------------------------
#### launch app locally
runApp()
## --------------------------------------
#### launch app locally
runApp()
library(shiny)
library(sveval)
library(VariantAnnotation)
## --------------------------------------
#### launch app locally
runApp()
#### launch app on UCSC server
#runApp(port=3457, host='0.0.0.0')
library(shiny)
library(sveval)
library(VariantAnnotation)
## --------------------------------------
#### launch app locally
runApp()
#### launch app on UCSC server
#runApp(port=3457, host='0.0.0.0')
library(shiny)
library(sveval)
library(VariantAnnotation)
## --------------------------------------
#### launch app locally
runApp()
#### launch app on UCSC server
#runApp(port=3457, host='0.0.0.0')
